{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度置信网络（DBN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将若干个RBM“串联”起来则构成了一个DBN，其中，上一个RBM的隐层即为下一个RBM的显层，上一个RBM的输出即为下一个RBM的输入。训练过程中，需要充分训练上一层的RBM后才能训练当前层的RBM，直至最后一层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='dbn01.png' width='60%'/>\n",
    "<center>__DBN的结构图__</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 该模型是logistic回归模型在多分类问题上的推广；\n",
    "* 在多分类问题中，类标签 $y$ 可以取两个以上的值；\n",
    "* Softmax回归是有监督的，不过后面也会介绍它与深度学习/无监督学习方法的结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回顾logistic回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们的测试集有 $m$ 个样本构成：$\\{(x^{(1)},y^{(1)}), …,(x^{(m)},y^{(m)})\\}$，因为logistic回归是针对二分类的模型，所以$y^{(i)}\\in \\{0, 1\\}$ <br>\n",
    "我们对符号的约定如下：特征向量 $x$ 的维度为 $n+1$，其中 $x_0 = 1$ 对应截距项 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic使用sigmoid函数 $sigmoid(x) = \\frac{1}{1+exp(-W^Tx)}$,$h_{\\theta}(x) = \\frac{1}{1+exp(-\\theta^Tx)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将训练模型参数 $\\theta$，使其能够最小化代价函数<br>\n",
    "代价函数：$J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在softmax回归中，类标签（相对于logistic二回归） $y$可以去 $k$个不同的值（$k > 2$）<br>\n",
    "在测试集$\\{(x^{(1)},y^{(1)}), …,(x^{(m)},y^{(m)})\\}$中，$y\\in \\{1, 2, …, k\\}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax回归模型代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax回归模型参数化的特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax回归与Logistic 回归的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
