{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 看一下这个博客https://www.cnblogs.com/jhding/p/5687696.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 受限玻尔兹曼机\n",
    "\n",
    "> 受限玻尔兹曼机（Restricted Boltzmann Machine，简称RBM）\n",
    "\n",
    "## RBM的结构\n",
    "&emsp;&emsp;限制玻尔兹曼机（RBM）来源于玻尔兹曼机（BM），之所以被加上“限制”两字，是因为它并不允许同一层中神经元的连接。神经元的输出只有两种状态，一般使用二进制的0和1表示。限制波尔兹曼机的结构如下图所示：\n",
    "\n",
    "<img src=\"20140913095932039.png\" width = \"60%\" />\n",
    "\n",
    "&emsp;&emsp;RBM有一个可见层和一个隐藏层。RBM具有很好的性质：在给定可见层单元状态（输入数据）时，各隐藏层单元的激活与否是条件独立的；反之，给定隐藏层单元的状态时，可见层单元的激活与否亦条件独立。<br>\n",
    " \n",
    "&emsp;&emsp;RBM的快速学习算法----对比分歧算法是由Hinton在“A fast learning algorithm for deep belief nets”中所提出的。\n",
    "\n",
    "<img src=\"21110338-415413c3686645a890cb490a36f8ef70.png\" width = \"60%\" />\n",
    "\n",
    "&emsp;&emsp;RBM是一种基于能量的模型，其可见变量v和隐藏变量h的联合配置的能量为：\n",
    "$$E(v,h;\\theta) = -\\sum_{i=1}^n a_iv_i - \\sum_{j=1}^m b_jh_j - \\sum_{i=1}^n\\sum_{j=1}^m v_iW_{ij}h_j$$\n",
    "&emsp;&emsp;上式中，$\\theta = \\{W_{ij},a_i,b_j\\}$是RBM的参数，它们均为实数。<br>\n",
    "\n",
    "&emsp;&emsp;当参数确定时，基于该能量函数，可以得到$(v,h)$的联合概率分布：\n",
    "$$P(v,h;\\theta) = \\frac{e^{-E(v,h;\\theta)}}{Z(\\theta)},\\qquad Z(\\theta)=\\sum_{v,h}e^{-E(v,h;\\theta)}$$\n",
    "&emsp;&emsp;上式中$Z$是被称为配分函数的归一化常数。<br>\n",
    "\n",
    "&emsp;&emsp;由RBM的特殊结构可知：当给定可见层单元的状态时，各隐藏层的激活状态之间的条件是独立的。此时，第$j$个隐藏层的激活概率为:\n",
    "$$P(h_j=1|v,\\theta)=\\sigma(b_j+\\sum_i v_iW_{ij})$$\n",
    "\n",
    "&emsp;&emsp;由于RBM的结构式对称的，当给定隐藏层单元的状态时，各可见层单元的激活状态之间也是条件独立的，即第$i$个可见层单元的激活概率为：\n",
    "$$P(v_i=1|h,\\theta)=\\sigma(a_i+\\sum_jW_{ij}h_i)$$\n",
    "\n",
    "## RBM的学习算法\n",
    "&emsp;&emsp;最常规的方法是借助极大似然法来对参数的更新规则进行推到，但是对极大似然函数的log形式进行推到后发现，梯度是很难计算的。<br>\n",
    "&emsp;&emsp;RBM的快速学习算法----对比分歧算法是由Hinton在“A fast learning algorithm for deep belief nets”中所提出的。在对比分歧算法被提出之后，Deep Learning才真正有了它的用武之地，在学术界与工业界才有了如今这么好的发展。<br>\n",
    "&emsp;&emsp;对比分歧算法中，当使用训练数据初始化时，仅需要使用1步Gibbs采样便可以得到足够好的近似。在算法一开始，可见单元的状态被设置成一个训练样本，并利用公式计算隐藏层单元的二值状态。在所有隐藏层单元的状态确定后，根据公式来确定第i个可见单元取值为1的概率，进而产生可见层的一个重构。算法的具体步骤为：\n",
    "* $输入：一个训练样本x_0；隐层单元个数m；学习率\\epsilon；最大训练周期T。$\n",
    "* $输出：连接权重矩阵W、可见层的偏置向量a、隐层的偏置向量b。$\n",
    "* 训练阶段：\n",
    "* $初始化：令可见单元的初始状态v_1 = x_0;W、a和b为随机的较小数值。$<br>\n",
    "    $For\\ t = 1,2,…,T $<br>\n",
    "    $\\qquad For\\ j=1,2…,m（对所有隐单元）$<br>\n",
    "    $\\qquad\\qquad计算P(h_{1j}=1|v_1),即P(h_{1j}=1|v_1)=\\sigma(b_j+\\sum_{i}v_{1i}W_{ij});\\\\\n",
    "     \\qquad\\qquad从条件分布P(h_{1j}|v_1)中抽取h_{1j}\\in\\{0,1\\};$<br>\n",
    "    $\\qquad EndFor$<br>\n",
    "    $\\qquad For\\ i=1,2…,n（对所有可见单元）$<br>\n",
    "    $\\qquad\\qquad计算P(v_{2i}=1|h_1),即P(v_{2i}=1|h_1)=\\sigma(a_i+\\sum_{j}W_{ij}h_{1j});\\\\\n",
    "     \\qquad\\qquad从条件分布P(v_{2i}|h_1)中抽取v_{2i}\\in\\{0,1\\};$<br>\n",
    "    $\\qquad EndFor$<br>\n",
    "    $\\qquad For\\ j=1,2…,m（对所有隐单元）$<br>\n",
    "    $\\qquad\\qquad计算P(h_{2j}=1|v_2),即P(h_{2j}=1|v_2)=\\sigma(b_j+\\sum_{i}v_{2i}W_{ij});$<br>\n",
    "    $\\qquad EndFor$<br>\n",
    "    $\\qquad 各个参数更新$<br>\n",
    "    $\\qquad W:=W + \\epsilon(P(h_1=1|v_1)v_1^T - P(h_2=1|v_2)v_2^T);\\\\\n",
    "     \\qquad a:=a + \\epsilon(v_1 - v_2);\\\\\n",
    "     \\qquad b:=b + \\epsilon(P(h_1=1|v_1) - P(h_2=1|v_2));$<br>\n",
    "    $EndFor$\n",
    "\n",
    "&emsp;&emsp;要想使一个模型取得好的效果，一个好的学习算法固然不可少，当然模型参数的选择也是很重要的。接下来就针对RBM说说参数的选择中应该注意到的问题。<br>\n",
    "&emsp;&emsp;学习率：学习率若过大，将导致重构误差急剧增加，权值也会变得异常大。设置学习率的一般做法是先做权重更新和权重的直方图，令权重更新量为权重的$10^-3$倍左右。如果有一个单元的输入值很大，则权重更新应再小一些，因为同一方向上较多的小的波动很容易改变梯度的符号。相反的，对于偏置，其权重更新可以大一些。<br>\n",
    "&emsp;&emsp;权重和偏置的初始值：一般的连接权重$W_{ij}$可初始化为来自正太分布$N(0,0.01)$的随机数，隐藏层单元的偏置$b_j$初始化为$0$。对于第$i$个可见单元，其偏置$a_i$通常初始化为$log[p_i/(1-p_i)]$，其中$p_i$表示训练样本中第$i$个特征处于激活状态所占的比率。<br>\n",
    "&emsp;&emsp;隐藏层单元个数：如果我们关心的主要目标是避免过拟合而不是计算复杂度，则可以先估算一下用一个好的模型描述一个数据所需要的比特数，用其乘上训练集容量。基于所得的数，选择比其低一个数量级的值作为隐藏层单元的个数。如果训练数据是高度冗余的，则可以使用更少一些的隐藏层单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过代码实现RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入需要的包\n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义sigmoid函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pa(pi):\n",
    "    return np.log10(pi/(1-pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 10 # 显单元个数\n",
    "n = 5 # 隐单元个数\n",
    "epasilon = 0.001 # 学习率\n",
    "T = 1000 # 训练周期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.randint(0, 2, size=[m, 1])\n",
    "v1 = x0\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99254061],\n",
       "       [-0.64036546],\n",
       "       [-0.27734497],\n",
       "       [-0.18194869],\n",
       "       [ 0.06756753],\n",
       "       [-0.14161512],\n",
       "       [ 0.33782189],\n",
       "       [-0.59007436],\n",
       "       [ 0.85761102],\n",
       "       [-1.55038594]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.random.random((m, 1))\n",
    "a = pa(pi)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00859907,  0.01772608, -0.01110363,  0.00181214,  0.00564345],\n",
       "       [-0.0056651 ,  0.00729976,  0.00372994,  0.00533811, -0.00091973],\n",
       "       [ 0.0191382 ,  0.00330797,  0.01141943, -0.01129595, -0.00850052],\n",
       "       [ 0.0096082 , -0.00217418,  0.00158515,  0.00873418, -0.00111383],\n",
       "       [-0.01038039, -0.0100948 , -0.01058257,  0.00656284, -0.00062492],\n",
       "       [-0.01738654,  0.00103163, -0.00621667,  0.00275718, -0.01090675],\n",
       "       [-0.00609985,  0.00306412,  0.01691826, -0.00747954, -0.00580797],\n",
       "       [-0.00110754,  0.02042029,  0.00447521,  0.00683384,  0.00022886],\n",
       "       [ 0.00857234,  0.00183931, -0.00416112,  0.0125005 ,  0.012483  ],\n",
       "       [-0.00757674,  0.00588294,  0.00346859,  0.01367033,  0.00673716]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = np.zeros((n, 1))\n",
    "b = np.zeros((n, 1))\n",
    "W = np.random.normal(loc=0, scale=0.01, size=[m, n])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[ 0.13365359,  0.1664602 ,  0.13370464,  0.14933299,  0.15104216],\n",
       "        [ 0.08671601,  0.10436098,  0.09770002,  0.10131914,  0.09377718],\n",
       "        [ 0.0191382 ,  0.00330797,  0.01141943, -0.01129595, -0.00850052],\n",
       "        [ 0.0096082 , -0.00217418,  0.00158515,  0.00873418, -0.00111383],\n",
       "        [-0.00352841,  0.0003285 , -0.00302841,  0.01533603,  0.00762322],\n",
       "        [ 0.01386806,  0.0359145 ,  0.02587074,  0.03618664,  0.02192306],\n",
       "        [ 0.00075213,  0.01348742,  0.02447242,  0.00129365,  0.00244016],\n",
       "        [ 0.08183037,  0.10776777,  0.08885887,  0.09310896,  0.08535402],\n",
       "        [ 0.01542432,  0.0122626 ,  0.00339304,  0.02127369,  0.02073113],\n",
       "        [-0.00757674,  0.00588294,  0.00346859,  0.01367033,  0.00673716]]),\n",
       " 'a': array([[-0.73154061],\n",
       "        [-0.47036546],\n",
       "        [-0.27734497],\n",
       "        [-0.18194869],\n",
       "        [ 0.06756753],\n",
       "        [-0.09261512],\n",
       "        [ 0.33782189],\n",
       "        [-0.43807436],\n",
       "        [ 0.85761102],\n",
       "        [-1.55038594]]),\n",
       " 'b': array([[ 0.00685198],\n",
       "        [ 0.0104233 ],\n",
       "        [ 0.00755416],\n",
       "        [ 0.00877319],\n",
       "        [ 0.00824813]])}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def p_b(p):\n",
    "    _p = np.array(p)\n",
    "    _p[_p <= 0.5] = 0\n",
    "    _p[_p > 0.5] = 1\n",
    "    return _p\n",
    "\n",
    "# \n",
    "for i in range(T):\n",
    "    p_h1_v1 = sigmoid(b + v1.T.dot(W).T)\n",
    "    h1 = p_b(p_h1_v1)\n",
    "    \n",
    "    p_v2_h1 = sigmoid(a + W.dot(h1))\n",
    "    v2 = p_b(p_v2_h1)\n",
    "    \n",
    "    p_h2_v2 = sigmoid(b + v2.T.dot(W).T)\n",
    "    \n",
    "    W = W + epasilon * (v1.dot(p_h1_v1.T) - v2.dot(p_h2_v2.T))\n",
    "    a = a + epasilon * (v1 - v2)\n",
    "    b = b + epasilon * (p_h1_v1 - p_h2_v2)\n",
    "    \n",
    "parameter = {'W': W, 'a': a, 'b': b}\n",
    "parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
