{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超图（HyperGraph）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的来说，对于我们熟悉的图而言，它的一个边（edge）只能和两个顶点连接；而对于超图来讲，人们定义它的边（这里叫超边，hyperedge）可以和任意个数的顶点连接。一个图和超图的示意图如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='graph01.gif' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='graph02.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个超图$G=(V,E,w)$，$V$ 是一个有限的顶点集，$E$ 是超图的超边集。<br>\n",
    "任意一个超边 $e \\in E$ 是顶点集 $V$的一个子集，$\\cup_{e \\in E}=V$，并被赋予一个非负权重 $w(e)$。<br>\n",
    "对于任意顶点 $v \\in V$，它的度可以定义为 $d(v)=\\sum_{\\{e\\in E | v\\in e\\}}w(e)$<br>\n",
    "对于超边 $e \\in E$，它的度可以定义为边上所有顶点的数目，$\\delta(e)=|e|$<br>\n",
    "方阵 $D_v$、$D_e$、$W$分别定义为顶点度、超边度和超边权重的对角矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超图的结构可以通过 $|V| \\times |E|$ 维的点边关联矩阵 $H$ 来进行描述。<br>\n",
    "如果一个顶点 $v$ 在一个超边 $e$ 上，其第 $(v, e)$个元素被赋值为$1$，$h(v,e)=1$，反之，$h(v,e)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据关联矩阵的定义<br>\n",
    "顶点度的定义可以为 $d(v)=\\sum_{e\\in E}w(e)h(v,e)$<br>\n",
    "超边度的定义可以为 $\\delta(e)=\\sum_{v\\in V}h(v,e)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 周氏标准化拉普拉斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "周氏标准化拉普拉斯是一种非常经典的超图算法。目前该算法在计算机领域非常流行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta=I-D_v^{-\\frac{1}{2}}HWD_e^{-1}H^TD_v^{-\\frac{1}{2}}$ 其中，矩阵 $I$ 是一个单位阵，而矩阵 $\\Delta$ 便是标准化超图拉普拉斯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化损失函数定义如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\Omega(f) & = \\frac{1}{2}\\sum_{e\\in E} \\sum_{u,v\\in V}\\frac{w(e)h(u,e)h(v,e)}{\\delta(e)}(\\frac{f(u)}{\\sqrt{d(u)}}-\\frac{f(v)}{\\sqrt{d(v)}})^2 \\\\\n",
    "   & = \\sum_{e\\in E} \\sum_{u,v\\in v}\\frac{w(e)h(u,e)h(v,e)}{\\delta(e)}(\\frac{f^2(u)}{d(u)}-\\frac{f(u)f(v)}{\\sqrt{d(u)d(v)}})\\\\\n",
    "   & = \\sum_{u\\in V}f^2(u)\\sum_{e\\in E}\\frac{w(e)h(u,e)}{d(u)}\\sum_{v\\in V}\\frac{h(v,e)}{\\delta(e)}-\\sum_{e\\in E}\\sum_{u.v\\in V}\\frac{f(u)h(u,e)w(e)h(v,e)f(v)}{\\sqrt{d(u)d(v)}\\delta(e)}\\\\\n",
    "   & = f^Tf-f^TD_v^{-\\frac{1}{2}}HWD_e^{-1}H^TD_v^{-\\frac{1}{2}}f\\\\\n",
    "   & = f^T(I-D_v^{-\\frac{1}{2}}HWD_e^{-1}H^TD_v^{-\\frac{1}{2}})f\\\\\n",
    "   & = f^T\\Delta f\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超图算法主要是用于无监督学习（Unsupervised Learning）和半监督学习（Semi-supervised Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于无监督学习用到的损失函数：$\\breve{f}=argmin\\Omega(f):=f^T\\Delta f$ 可以通过特征值分解求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于半监督学习用到的损失函数：$\\breve{f}=argmin\\Phi(f):=f^T\\Delta f+\\mu \\|f-y\\|^2$<br>\n",
    "该问题通过最小二乘法求解：$f=\\mu(\\Delta+\\mu I)^{-1}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildHyperGraph(dist, k):\n",
    "    \n",
    "    m = dist.shape[0]\n",
    "    # 超图的关联矩阵\n",
    "    hypergraph = np.zeros((m, m))\n",
    "    \n",
    "    median = np.median(dist)\n",
    "    \n",
    "    affinity = np.exp(-dist / median)\n",
    "    affinity[affinity == 1] = 0\n",
    "    \n",
    "    # 降序排序\n",
    "    affinity_sort = -np.sort(-affinity, axis=1)\n",
    "    \n",
    "    affinity_arg = np.argsort(-affinity, axis=1)\n",
    "    \n",
    "    we = np.sum(affinity_sort[:, 0:k], axis=1)\n",
    "    \n",
    "    klist = affinity_arg[:, 0:k]\n",
    "    \n",
    "    for i in range(m):\n",
    "        hypergraph[i, klist[i, :]] = 1\n",
    "    \n",
    "    De = np.diag(hypergraph.sum(axis=0))\n",
    "    \n",
    "    Dv = np.diag(we.dot(hypergraph))\n",
    "    \n",
    "    We = np.diag(we)\n",
    "    \n",
    "    return hypergraph, We, Dv, De\n",
    "\n",
    "\n",
    "def theta(H, W, Dv, De):\n",
    "    sqrt_inv_Dv = np.sqrt(lg.inv(Dv))\n",
    "    return sqrt_inv_Dv.dot(H).dot(W).dot(lg.inv(De)).dot(H.T).dot(sqrt_inv_Dv)\n",
    "    \n",
    "    \n",
    "def ranking_display(dist, k=3, u=9):\n",
    "    # 距离矩阵\n",
    "    m = dist.shape[0]\n",
    "    H, We, Dv, De = buildHyperGraph(dist, k)\n",
    "    t = theta(H, We, Dv, De)\n",
    "    I = np.eye(m, m)\n",
    "    f = np.zeros((m, 1))\n",
    "    y = np.zeros((m, 1))\n",
    "       \n",
    "    p = np.random.randint(m, size=(m, 1))\n",
    "    \n",
    "    for i in range(m):\n",
    "        y[:] = 0\n",
    "        y[p[i]] = 1\n",
    "        f = lg.inv((I - 1 / (1 + u) * t)).dot(y) \n",
    "        print(-np.sort(-f,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = np.random.rand(16, 16) * 255\n",
    "ranking_display(dist, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
